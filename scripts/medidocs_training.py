# # -*- coding: utf-8 -*-
# """medidocs_training.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1cBfWZYLlXR11T_u6T4exwI_CGLOtYUPt
# """

# from google.colab import drive
# drive.mount('/content/drive')

# !pip install simpletransformers transformers datasets tqdm pandas
# !pip install rouge-score==0.0.4
# !pip install docx
# !pip install exceptions
# !pip install python-docx

# ! git config --global credential.helper store

# # Commented out IPython magic to ensure Python compatibility.
# # %%capture
# # ! sudo apt-get install git-lfs

import os
from docx import Document
from docx.shared import Pt
import docx
import re
from docx.enum.text import WD_ALIGN_PARAGRAPH
import glob
import time
import pandas as pd
from datetime import datetime
from transformers import AutoTokenizer, T5ForConditionalGeneration
from sklearn.model_selection import train_test_split
import pyarrow as pa
import pyarrow.dataset as ds
from datasets import Dataset
from transformers import Seq2SeqTrainer
from transformers import Seq2SeqTrainingArguments
from datasets import load_metric
#from huggingface_hub.hf_api import HfFolder 
#import wandb
import torch, gc
import sys
import datetime
#************ passed args ***************
dir_in = sys.argv[1]
dir_out = sys.argv[2]
dir_model = sys.argv[3]
num_v = int(sys.argv[4])

Read_from_input_parent_dir=dir_in
Read_from_target_parent_dir=dir_out
Load_from_models_dir=dir_model
Model_Selected=num_v# from front end


# Read_from_input_parent_dir="/content/drive/MyDrive/NLPverse/Medidocx/Project Development /Training /Input"  # from front end
# Read_from_target_parent_dir="/content/drive/MyDrive/NLPverse/Medidocx/Project Development /Training /Target"  # from front end
# Load_from_models_dir="/content/drive/MyDrive/NLPverse/Medidocx/Project Development /Models"  # from front end
# Model_Selected=0 # from front end

#***** moved all functions ******
def check_header(pattern,text):
  for pattern in date_formats:
    match = re.search(pattern, text)
    if match:
      return match.group()
    return -1 #not found
      
def process_header(text):
  parts = re.split("[,]", text)
  names = []
  p=0
  for part in parts:
    if part:
      p+=1
      # add the part to the list of names
      if p==1:
        part=part[1:].lstrip()
      if p==2: # doctor
        part = part.lstrip()
        if part[:2]=="MD":
          part=part[2:].lstrip() + ", MD"
      names.append(part.lstrip())
  
  headers=["","",""]
  if len(parts)<2:
    headers[1]="Unknown" #MD

  headers[0]= names[0]
  headers[1]=names[1] #MD
  headers[2]= ", ".join(names[2:]) #other info
  return headers

def doc_to_list(document):
  out_paraph=[]
  out_type=[]
  start=0
  for paragraph in document.paragraphs:
      raw_input=paragraph.text
      clean_input = re.sub(r'[^\x00-\x7F]+','', raw_input).strip()
      if len(clean_input)>0:
        if len(clean_input.strip())>=10:
          date_possible_loc=clean_input[:10].strip()
        else:
          date_possible_loc=clean_input

        if not(check_header(date_formats,date_possible_loc)== -1) and not(start):
          start=1

        if start:
          if len(clean_input)==0 or len(clean_input)>512:
            out_type.append("skipped")
            out_paraph.append("")
            continue

          if not(check_header(date_formats,date_possible_loc)== -1):
            date=check_header(date_formats,clean_input) # header
            out_paraph.append(clean_input)
            out_type.append("header")

          else: 
            out_paraph.append(clean_input) 
            out_type.append("text")
  return out_paraph,out_type

def get_latest_subdir(Load_from_models_dir,Model_Selected):
  # Get a list of all subdirectories in the directory
    models_folders =[entry.name for entry in os.scandir(Load_from_models_dir) if entry.is_dir() and entry.name.startswith('T5-')]
    # Sort the list of subdirectories by the date they were created
    models_folders.sort(key=lambda x: os.path.getctime(os.path.join(Load_from_models_dir, x)), reverse=True)
    # Print the list of subdirectories
    model_dir=os.path.join(Load_from_models_dir,models_folders[Model_Selected])
    # Get a list of all subdirectories in the given directory
    #subdirs = [os.path.join(dir_path, d) for d in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path, d))]
    #subdirs.sort(key=lambda x: datetime.fromtimestamp(os.path.getmtime(x)))
    #return subdirs[-1]
    return model_dir

def format_data(input, target):
  return {'input': input, 'target': target}

def map_to_length(x):
  x["input_len"] = len(tokenizer(x["input"]).input_ids)
  x["input_longer_256"] = int(x["input_len"] > 256)
  x["input_longer_128"] = int(x["input_len"] > 128)
  x["input_longer_64"] = int(x["input_len"] > 64)
  x["out_len"] = len(tokenizer(x["target"]).input_ids)
  x["out_longer_256"] = int(x["out_len"] > 256)
  x["out_longer_128"] = int(x["out_len"] > 128)
  x["out_longer_64"] = int(x["out_len"] > 64)
  return x

def compute_and_print_stats(x):
  if len(x["input_len"]) == sample_size:
    print(
        "Input Mean: {}, %-Input > 256:{},  %-Input > 128:{}, %-Input > 64:{} Output Mean:{}, %-Output > 256:{}, %-Output > 128:{}, %-Output > 64:{}".format(
            sum(x["input_len"]) / sample_size,
            sum(x["input_longer_256"]) / sample_size,
            sum(x["input_longer_128"]) / sample_size,
            sum(x["input_longer_64"]) / sample_size,   
            sum(x["out_len"]) / sample_size,
            sum(x["out_longer_256"]) / sample_size,
            sum(x["out_longer_128"]) / sample_size,
            sum(x["out_longer_64"]) / sample_size,
        )
    )

def convert_to_features(example_batch):
    input_encodings = tokenizer.batch_encode_plus(example_batch['input'], pad_to_max_length=True, max_length=200)
    target_encodings = tokenizer.batch_encode_plus(example_batch['target'], pad_to_max_length=True, max_length=512)

    encodings = {
        'input_ids': input_encodings['input_ids'], 
        'attention_mask': input_encodings['attention_mask'],
        'labels': target_encodings['input_ids'],
        'decoder_attention_mask': target_encodings['attention_mask']
    }

    return encodings

def compute_metrics(pred):
    labels_ids = pred.label_ids
    pred_ids = pred.predictions
    # all unnecessary tokens are removed
    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    labels_ids[labels_ids == -100] = tokenizer.pad_token_id
    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)

    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=["rouge2"])["rouge2"].mid
    return {
        "rouge2_precision": round(rouge_output.precision, 4),
        "rouge2_recall": round(rouge_output.recall, 4),
        "rouge2_fmeasure": round(rouge_output.fmeasure, 4),
        # "accuracy": acc_out,
    }

def paraphrase_med(text):
    inputs = tokenizer(text, padding='longest', max_length=256, return_tensors='pt')
    input_ids = inputs.input_ids
    attention_mask = inputs.attention_mask
    output = model.generate(input_ids, attention_mask=attention_mask, max_length=256)
    return tokenizer.decode(output[0], skip_special_tokens=True)

def move_files_to_dated_subdir(source_dir):
    # Get the current date in the YYYY-MM-DD format
    date = datetime.datetime.now().strftime("%Y-%m-%d")

    # Append the current date to the source directory path to create the target directory
    target_dir = os.path.join(source_dir, "Archive/"+date)
    if not os.path.exists(target_dir):
        os.mkdir(os.path.join(source_dir, "Archive"))
        os.mkdir(target_dir)

    for file in os.listdir(source_dir):
        if os.path.isfile(os.path.join(source_dir, file)):
            os.rename(os.path.join(source_dir, file), os.path.join(target_dir, file))
#***** end of the functions *******

# Set the pattern for document files
pattern = '*.doc*'
input_files = glob.glob(os.path.join(Read_from_input_parent_dir, pattern)) 
target_files = glob.glob(os.path.join(Read_from_target_parent_dir, pattern))

# Get a list of all subdirectories in the directory
models_folders =[entry.name for entry in os.scandir(Load_from_models_dir) if entry.is_dir() and entry.name.startswith('T5-')]
# Sort the list of subdirectories by the date they were created
models_folders.sort(key=lambda x: os.path.getctime(os.path.join(Load_from_models_dir, x)))
model_dir=os.path.join(Load_from_models_dir,models_folders[Model_Selected])

# Get the current date and time as a tuple
now = time.localtime()

# Format the date as a string in the desired format
date_str = time.strftime("%d-%m-%y", now)

Model_Name="T5-paraphrase-PAW-"+date_str
Model_saveto=os.path.join(Load_from_models_dir,Model_Name)

"""# Preparing Dataframe For matching file names"""

# Create a data frame for the input files
input_df = pd.DataFrame({"input_file": input_files})

# Create a data frame for the target files
target_df = pd.DataFrame({"target_file": target_files})

# Extract the file names from the input and target file paths
input_df['input_file_name'] = input_df['input_file'].apply(lambda x: x.split('/')[-1])
target_df['target_file_name'] = target_df['target_file'].apply(lambda x: x.split('/')[-1])

# Merge the two data frames on the file name columns
merged_df = pd.merge(input_df, target_df, left_on='input_file_name', right_on='target_file_name')

# Drop the file name columns
merged_df = merged_df[['input_file', 'target_file']]

"""## Reading Methods"""

date_formats = [
    r"\d\d/\d\d/\d\d\d\d",  # DD/MM/YYYY (e.g. "12/12/2022")
    r"\d\d/\d\d/\d\d",  # DD/MM/YY_ (e.g. "12/12/2022")
    r"\d\d/\d\d/\d\d",       # DD/MM/YY (e.g. "12/12/22")
    r"\d\d-\d\d-\d\d\d\d",  # DD-MM-YYYY (e.g. "12-12-2022")
    r"\d\d-\d\d-\d\d",  # DD-MM-YYYY (e.g. "12-12-22")
    r"\d\d\d\d-\d\d-\d\d",  # YYYY-MM-DD (e.g. "2022-12-12")
    r"\d\d\d\d/\d\d/\d\d",  # YYYY/MM/DD (e.g. "2022/12/12")
    r"\d\d-\w\w\w-\d\d\d\d", # DD-Mon-YYYY (e.g. "12-Dec-2022")
    r"\d\d \w\w\w+ \d\d", # DD Month YYYY (e.g. "12 December 2022")
    r"\w\w\w \d\d \d\d\d\d", # Mon DD YYYY (e.g. "Dec 12 2022")
    r"\d\d \w\w\w+ \d\d\d\d", # DD Month YYYY (e.g. "12 December 2022")
    r"\d\d \w\w\w+ \d\d", # DD Month YYYY (e.g. "12 December 22")
]

df_training = pd.DataFrame(columns=['input', 'target'])
for index, row in merged_df.iterrows():
    input_file = row['input_file']
    target_file = row['target_file']

    file_name=os.path.basename(input_file)
    input_document = docx.Document(input_file)
    target_document = docx.Document(target_file)
    in_text,in_type= doc_to_list(input_document)
    out_text,out_type= doc_to_list(target_document)

    tolerance=2
    if abs(len(in_text) - len(out_text)) > tolerance: continue
    # Create a data frame from the lists
    df = pd.DataFrame({"input": in_text, "in_type": in_type, "target": out_text, "out_type": out_type})

    # Create a boolean mask that identifies rows with a mismatch between the "in_type" and "out_type" columns
    mask = df['in_type'] != df['out_type']
    # Drop rows with a mismatch between the "in_type" and "out_type" columns
    df = df.drop(df[mask].index)


    # Drop nans and rows with "skipped" or "header" in the "in_type" or "out_type" columns
    df = df[~((df['in_type'] == 'skipped') | (df['in_type'] == 'header') | (df['out_type'] == 'skipped') | (df['out_type'] == 'header'))]
    df = df.dropna()

    # Drop the "in_type" and "out_type" columns from the data frame
    df = df.drop(columns=['in_type', 'out_type'])
    df_training = pd.concat([df_training, df])

if len(df_training)<100:
  print("Not enough data to retrain")
if len(df_training)==0:
  raise ValueError("0 data matched data found")

most_recent_dir = get_latest_subdir(Load_from_models_dir,Model_Selected)
print(f"loading: {most_recent_dir}")




CKPT = most_recent_dir
tokenizer = AutoTokenizer.from_pretrained(CKPT)
model = T5ForConditionalGeneration.from_pretrained(CKPT)

inputs_org= df_training.input.values
outs_org= df_training.target.values
inputs_fixed=[]
for index in inputs_org:
    full = 'paraphrase: ' + index
    inputs_fixed.append(full)

x_train, x_test, y_train, y_test = train_test_split(inputs_fixed, outs_org, test_size=0.2, random_state=0)

training_data = format_data(x_train, y_train)
testing_data = format_data(x_test, y_test)

df = pd.DataFrame({'input': training_data["input"], 'target': training_data["target"]})
dataset = ds.dataset(pa.Table.from_pandas(df).to_batches())

hg_train = Dataset(pa.Table.from_pandas(df))

df1 = pd.DataFrame({'input': testing_data["input"], 'target': testing_data["target"]})
dataset1 = ds.dataset(pa.Table.from_pandas(df1).to_batches())

### convert to Huggingface dataset
hg_test = Dataset(pa.Table.from_pandas(df1))

# map article and summary len to dict as well as if sample is longer than 512 tokens
#changed sample size from 20
sample_size = 20
data_stats = hg_train.select(range(sample_size)).map(map_to_length, num_proc=4)

output = data_stats.map(
  compute_and_print_stats, 
  batched=True,
  batch_size=-1,
)

"""###Data Tokenization"""

# tokenize the examples
hg_train = hg_train.map(convert_to_features, batched=True)
hg_test = hg_test.map(convert_to_features, batched=True)

columns = ['input_ids', 'attention_mask', 'labels', 'decoder_attention_mask']

hg_train.set_format(type='torch', columns=columns)
hg_test.set_format(type='torch', columns=columns)

# set training arguments - Feel free to adapt it
training_args = Seq2SeqTrainingArguments(
    output_dir=Model_saveto,
    per_device_train_batch_size=8,
    num_train_epochs=1,
    learning_rate=0.0001,
    per_device_eval_batch_size=8,
    predict_with_generate=True,
    evaluation_strategy="epoch",
    do_train=True,
    do_eval=True,
    logging_steps=500,
    save_strategy="epoch",
    #save_steps=1000,
    #eval_steps=1000,
    overwrite_output_dir=True,
    save_total_limit=1,
    load_best_model_at_end=True,
    push_to_hub=False
    #fp16=True, 
)

rouge = load_metric("rouge")

#HfFolder.save_token('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')

# instantiate trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=hg_train,
    eval_dataset=hg_test
)

#os.environ["WANDB_API_KEY"] = "278a845bcf5619b0dccc2d7c111763bae480e2ba"
#os.environ["WANDB_MODE"] = "offline" 
#os.environ["WANDB_DISABLED"] = "true"
# wandb.init()
# wandb.run.name = wandb.run.id
# wandb.run.save()

trainer.evaluate()

gc.collect()
torch.cuda.empty_cache()

trainer.train()

"""# Create Training"""
if not os.path.exists(Model_saveto):
  print(f"created new dir: {Model_saveto}")
  os.makedirs(Model_saveto)
else:
  print(f"overwriting: {Model_saveto}")

trainer.save_model(Model_saveto)

tokenizer.save_pretrained(Model_saveto)

# trainer.create_model_card()
print(f"saving to: {Model_saveto}")
inp_path = Read_from_input_parent_dir
tar_path = Read_from_target_parent_dir
move_files_to_dated_subdir(inp_path)
move_files_to_dated_subdir(tar_path)

